# -*- coding: utf-8 -*-
"""Stock-Market.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UIgDIrQiNZy1Y0md-gomgAkjP8oDK7iC

**Name:Yashodeep Narendra Sonar**
**Project Title: Stock-Market Analysis**
---
The "Stock-Market" project aims to analyze and compare the historical stock prices of Apple, Microsoft, Netflix, and Google over the past three months. Utilizing Python, SQL, machine learning, and Excel, the project seeks to identify trends, calculate moving averages, assess volatility, and conduct correlation analysis among the companies' stock performances. The comprehensive approach involves defining objectives, collecting and preparing data, performing exploratory data analysis (EDA), feature engineering, selecting appropriate machine learning models, training and evaluating these models, and final deployment.

# Step 1: Import Necessary Libraries
**pandas**: For data manipulation and analysis.

**numpy**: For numerical computations.

**matplotlib**.pyplot: For data visualization.

**seaborn**: For enhanced data visualizations.

**sklearn**: For implementing machine learning models.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler

"""# Step 2: Data Collection
The dataset is loaded from a CSV file, where we can view the first few entries to understand its structure.
"""

data = pd.read_csv('stocks.csv')
print(data.head())

"""# Step 3: Data Preparation
**dropna()**: Removes any rows with missing values to ensure clean data.

**pd.to_datetime()**: Converts the 'Date' column from string format to datetime objects for time-series analysis
"""

data['Date'] = pd.to_datetime(data['Date'])
data.dropna(inplace=True)

data = pd.get_dummies(data, columns=['Ticker'], drop_first=True)

"""# Step 4: Exploratory Data Analysis (EDA)
A histogram is plotted to visualize the distribution of closing prices, which helps in understanding data ranges and frequencies.
"""

plt.figure(figsize=(12, 6))
sns.histplot(data['Close'], bins=30, kde=True)
plt.title('Distribution of Closing Prices')
plt.xlabel('Closing Price')
plt.ylabel('Frequency')
plt.show()

"""# Step 5: Correlation Analysis
A correlation matrix is generated to observe relationships between various stock prices, helping identify any potential dependencies.
"""

plt.figure(figsize=(10, 8))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm')
plt.title("Correlation Matrix")
plt.show()

"""# Step 6: Feature Engineering
The target variable 'Close' is separated from the features, ensuring a focus on relevant data for modeling.
"""

X = data.drop(['Date', 'Close'], axis=1)
y = data['Close']

"""#Step 7: Data Standardization
StandardScaler: Normalizes the feature data, which is crucial for many machine learning models to perform accurately.
"""

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

"""# Step 8: Train-Test Split and model Training
**train_test_split**: Divides the data into training (80%) and testing (20%) sets, helping to validate model performance on unseen data.
"""

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

"""# Step 9: Model Evaluation
Predictions are made on the test set, comparing the results to compute Mean Squared Error and R^2 Score, which help quantify model accuracy.
"""

y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred)
plt.xlabel('True Values')
plt.ylabel('Predictions')
plt.title('True Values vs Predictions')
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')
plt.show()

"""# Conclusion
This notebook provides a comprehensive approach to analyzing stock market data using Python. Future steps could include implementing additional machine learning techniques, such as Random Forest or Neural Networks, as well as conducting more in-depth analytical interpretations.
"""